{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.\n",
    "\n",
    "#### Stop words\n",
    "\n",
    "Stop words are the most common words in a language like \"the\", \"is\", \"a\". These words do not carry important meaning and are usually removed from texts.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class preprocessing:\n",
    "    # clean_stem_lemmatize_column: full clean up for a column\n",
    "    def clean_stem_lemmatize_column(self, dataframe, column, more_stop_words = [], drop_unnecessary_columns = False):\n",
    "        self.clean_column(dataframe, column, more_stop_words)\n",
    "        self.stem_column(dataframe, f\"{column}_clean\")\n",
    "        self.lemmatize_column(dataframe, f\"{column}_clean_stems\")\n",
    "        if drop_unnecessary_columns is True:\n",
    "            dataframe = dataframe.drop([f\"{column}_clean\", f\"{column}_clean_stems\"], axis=1)\n",
    "        return dataframe\n",
    "\n",
    "    # tokenize_sentence: tokenizes a given sentence\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        words = word_tokenize(sentence)\n",
    "        return words\n",
    "\n",
    "    # tokenize_column: tokenizes a column of a dataframe and saves it as column_tokens\n",
    "    def tokenize_column(self, dataframe, column):\n",
    "        tokens = list()\n",
    "        # tokenize each element of the column\n",
    "        for index, row in dataframe.iterrows():\n",
    "            sentence_tokens = self.tokenize_sentence(dataframe.loc[index,column])\n",
    "            tokens.append(sentence_tokens)\n",
    "        # save tokens as a new column\n",
    "        dataframe[f\"{column}_tokens\"] = pd.Series(tokens, index=dataframe.index)\n",
    "\n",
    "    # clean_sentence: cleans a given sentence\n",
    "    def clean_sentence(self, sentence):\n",
    "        sentence = re.sub(\"[^a-zA-z\\s]\", \"\", sentence) # remove special characters\n",
    "        sentence = re.sub(\"_\", \"\", sentence)\n",
    "        sentence = re.sub(\"\\s+\", \" \",sentence)         # change any white space to one space\n",
    "        sentence = sentence.strip()                    # remove start and end white spaces\n",
    "        sentence = sentence.lower()                    # convert sentence into lower case\n",
    "        return sentence\n",
    "    \n",
    "    # remove_stop_words_from_sentence: removes stop words fast using dictionary\n",
    "    def remove_stop_words_from_sentence(self, sentence, more_stop_words = []):\n",
    "        stop_words = stopwords.words(\"english\") + more_stop_words\n",
    "        stopwords_dictionary = Counter(stop_words)\n",
    "        sentence = \" \".join([word for word in sentence.split() if word not in stopwords_dictionary])\n",
    "        return sentence\n",
    "\n",
    "    # clean_column: cleans a dataframe column from symbols, removes stop words and saves it as column_clean\n",
    "    def clean_column(self, dataframe, column, more_stop_words = []):\n",
    "        # clean and remove each element of the column\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_clean\"] = self.clean_sentence(dataframe.loc[index,column])\n",
    "            dataframe.loc[index,f\"{column}_clean\"] = self.remove_stop_words_from_sentence(dataframe.loc[index,f\"{column}_clean\"], more_stop_words)\n",
    "\n",
    "    # stem_sentence: stems a given sentence\n",
    "    def stem_sentence(self, sentence):\n",
    "        porter = PorterStemmer()\n",
    "        words = word_tokenize(sentence)\n",
    "        stems_sentence = list()\n",
    "        for word in words:\n",
    "            stems_sentence.append(porter.stem(word))\n",
    "        return \" \".join(stems_sentence)\n",
    "\n",
    "    # stem_column: stems a column of a dataframe and saves it as column_stems\n",
    "    def stem_column(self, dataframe, column):\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_stems\"] = self.stem_sentence(dataframe.loc[index,column])\n",
    "\n",
    "    # lemmatize_sentence: lemattizes a given sentence\n",
    "    def lemmatize_sentence(self, sentence):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = word_tokenize(sentence)\n",
    "        lemmas_sentence = list()\n",
    "        for word in words:\n",
    "            lemmas_sentence.append(lemmatizer.lemmatize(word))\n",
    "        return \" \".join(lemmas_sentence)\n",
    "\n",
    "    # lemmatize_sentence: lemattizes a column of a dataframe and saves it as column_lemmas\n",
    "    def lemmatize_column(self, dataframe, column):\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_lemmas\"] = self.lemmatize_sentence(dataframe.loc[index,column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_clean_stems</th>\n",
       "      <th>content_clean_stems_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>385</td>\n",
       "      <td>Tate &amp; Lyle boss bags top award\\n</td>\n",
       "      <td>\\n Tate &amp; Lyle's chief executive has been nam...</td>\n",
       "      <td>tate lyles chief executive named european busi...</td>\n",
       "      <td>tate lyle chief execut name european businessm...</td>\n",
       "      <td>tate lyle chief execut name european businessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1984</td>\n",
       "      <td>Halo 2 sells five million copies\\n</td>\n",
       "      <td>\\n Microsoft is celebrating bumper sales of i...</td>\n",
       "      <td>microsoft celebrating bumper sales xbox scifi ...</td>\n",
       "      <td>microsoft celebr bumper sale xbox scifi shoote...</td>\n",
       "      <td>microsoft celebr bumper sale xbox scifi shoote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>986</td>\n",
       "      <td>MSPs hear renewed climate warning\\n</td>\n",
       "      <td>\\n Climate change could be completely out of ...</td>\n",
       "      <td>climate change could completely control within...</td>\n",
       "      <td>climat chang could complet control within seve...</td>\n",
       "      <td>climat chang could complet control within seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1387</td>\n",
       "      <td>Pavey focuses on indoor success\\n</td>\n",
       "      <td>\\n Jo Pavey will miss January's View From Gre...</td>\n",
       "      <td>jo pavey miss januarys view great edinburgh in...</td>\n",
       "      <td>jo pavey miss januari view great edinburgh int...</td>\n",
       "      <td>jo pavey miss januari view great edinburgh int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1295</td>\n",
       "      <td>Tories reject rethink on axed MP\\n</td>\n",
       "      <td>\\n Sacked MP Howard Flight's local Conservati...</td>\n",
       "      <td>sacked mp howard flights local conservative as...</td>\n",
       "      <td>sack mp howard flight local conserv associ ins...</td>\n",
       "      <td>sack mp howard flight local conserv associ ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>439</td>\n",
       "      <td>German economy rebounds\\n</td>\n",
       "      <td>\\n Germany's economy, the biggest among the 1...</td>\n",
       "      <td>germanys economy biggest among countries shari...</td>\n",
       "      <td>germani economi biggest among countri share eu...</td>\n",
       "      <td>germani economi biggest among countri share eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>300</td>\n",
       "      <td>J&amp;J agrees $25bn Guidant deal\\n</td>\n",
       "      <td>\\n Pharmaceutical giant Johnson &amp; Johnson has...</td>\n",
       "      <td>pharmaceutical giant johnson johnson agreed bu...</td>\n",
       "      <td>pharmaceut giant johnson johnson agre buy medi...</td>\n",
       "      <td>pharmaceut giant johnson johnson agre buy medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>1286</td>\n",
       "      <td>Child access law shake-up planned\\n</td>\n",
       "      <td>\\n Parents who refuse to allow former partner...</td>\n",
       "      <td>parents refuse allow former partners contact c...</td>\n",
       "      <td>parent refus allow former partner contact chil...</td>\n",
       "      <td>parent refus allow former partner contact chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>1506</td>\n",
       "      <td>Real in talks over Gravesen move\\n</td>\n",
       "      <td>\\n Real Madrid are closing in on a Β£2m deal ...</td>\n",
       "      <td>real madrid closing deal evertons thomas grave...</td>\n",
       "      <td>real madrid close deal everton thoma gravesen ...</td>\n",
       "      <td>real madrid close deal everton thoma gravesen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2036</td>\n",
       "      <td>Lifestyle 'governs mobile choice'\\n</td>\n",
       "      <td>\\n Faster, better or funkier hardware alone i...</td>\n",
       "      <td>faster better funkier hardware alone going hel...</td>\n",
       "      <td>faster better funkier hardwar alon go help pho...</td>\n",
       "      <td>faster better funkier hardwar alon go help pho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                title  \\\n",
       "0     385    Tate & Lyle boss bags top award\\n   \n",
       "1    1984   Halo 2 sells five million copies\\n   \n",
       "2     986  MSPs hear renewed climate warning\\n   \n",
       "3    1387    Pavey focuses on indoor success\\n   \n",
       "4    1295   Tories reject rethink on axed MP\\n   \n",
       "..    ...                                  ...   \n",
       "440   439            German economy rebounds\\n   \n",
       "441   300      J&J agrees $25bn Guidant deal\\n   \n",
       "442  1286  Child access law shake-up planned\\n   \n",
       "443  1506   Real in talks over Gravesen move\\n   \n",
       "444  2036  Lifestyle 'governs mobile choice'\\n   \n",
       "\n",
       "                                               content  \\\n",
       "0     \\n Tate & Lyle's chief executive has been nam...   \n",
       "1     \\n Microsoft is celebrating bumper sales of i...   \n",
       "2     \\n Climate change could be completely out of ...   \n",
       "3     \\n Jo Pavey will miss January's View From Gre...   \n",
       "4     \\n Sacked MP Howard Flight's local Conservati...   \n",
       "..                                                 ...   \n",
       "440   \\n Germany's economy, the biggest among the 1...   \n",
       "441   \\n Pharmaceutical giant Johnson & Johnson has...   \n",
       "442   \\n Parents who refuse to allow former partner...   \n",
       "443   \\n Real Madrid are closing in on a Β£2m deal ...   \n",
       "444   \\n Faster, better or funkier hardware alone i...   \n",
       "\n",
       "                                         content_clean  \\\n",
       "0    tate lyles chief executive named european busi...   \n",
       "1    microsoft celebrating bumper sales xbox scifi ...   \n",
       "2    climate change could completely control within...   \n",
       "3    jo pavey miss januarys view great edinburgh in...   \n",
       "4    sacked mp howard flights local conservative as...   \n",
       "..                                                 ...   \n",
       "440  germanys economy biggest among countries shari...   \n",
       "441  pharmaceutical giant johnson johnson agreed bu...   \n",
       "442  parents refuse allow former partners contact c...   \n",
       "443  real madrid closing deal evertons thomas grave...   \n",
       "444  faster better funkier hardware alone going hel...   \n",
       "\n",
       "                                   content_clean_stems  \\\n",
       "0    tate lyle chief execut name european businessm...   \n",
       "1    microsoft celebr bumper sale xbox scifi shoote...   \n",
       "2    climat chang could complet control within seve...   \n",
       "3    jo pavey miss januari view great edinburgh int...   \n",
       "4    sack mp howard flight local conserv associ ins...   \n",
       "..                                                 ...   \n",
       "440  germani economi biggest among countri share eu...   \n",
       "441  pharmaceut giant johnson johnson agre buy medi...   \n",
       "442  parent refus allow former partner contact chil...   \n",
       "443  real madrid close deal everton thoma gravesen ...   \n",
       "444  faster better funkier hardwar alon go help pho...   \n",
       "\n",
       "                            content_clean_stems_lemmas  \n",
       "0    tate lyle chief execut name european businessm...  \n",
       "1    microsoft celebr bumper sale xbox scifi shoote...  \n",
       "2    climat chang could complet control within seve...  \n",
       "3    jo pavey miss januari view great edinburgh int...  \n",
       "4    sack mp howard flight local conserv associ ins...  \n",
       "..                                                 ...  \n",
       "440  germani economi biggest among countri share eu...  \n",
       "441  pharmaceut giant johnson johnson agre buy medi...  \n",
       "442  parent refus allow former partner contact chil...  \n",
       "443  real madrid close deal everton thoma gravesen ...  \n",
       "444  faster better funkier hardwar alon go help pho...  \n",
       "\n",
       "[445 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set  = pd.read_csv(\"./data_sets/test_set.tsv\", delimiter=\"\\t\",names=[\"id\",\"title\",\"content\"],           header=0)\n",
    "more_stop_words = [\"say\",\"said\",\"want\",\"thing\",\"may\",\"see\",\"make\",\"look\",\"likely\",\"well\",\"told\",\"uses\",\"used\",\"use\",\"bn\",\"mr\",\"year\",\"people\",\"new\"]\n",
    "pre = preprocessing()\n",
    "test_set = pre.clean_stem_lemmatize_column(test_set,\"content\",more_stop_words)\n",
    "display(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean_stems_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>385</td>\n",
       "      <td>Tate &amp; Lyle boss bags top award\\n</td>\n",
       "      <td>\\n Tate &amp; Lyle's chief executive has been nam...</td>\n",
       "      <td>tate lyle chief execut name european businessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1984</td>\n",
       "      <td>Halo 2 sells five million copies\\n</td>\n",
       "      <td>\\n Microsoft is celebrating bumper sales of i...</td>\n",
       "      <td>microsoft celebr bumper sale xbox scifi shoote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>986</td>\n",
       "      <td>MSPs hear renewed climate warning\\n</td>\n",
       "      <td>\\n Climate change could be completely out of ...</td>\n",
       "      <td>climat chang could complet control within seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1387</td>\n",
       "      <td>Pavey focuses on indoor success\\n</td>\n",
       "      <td>\\n Jo Pavey will miss January's View From Gre...</td>\n",
       "      <td>jo pavey miss januari view great edinburgh int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1295</td>\n",
       "      <td>Tories reject rethink on axed MP\\n</td>\n",
       "      <td>\\n Sacked MP Howard Flight's local Conservati...</td>\n",
       "      <td>sack mp howard flight local conserv associ ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>439</td>\n",
       "      <td>German economy rebounds\\n</td>\n",
       "      <td>\\n Germany's economy, the biggest among the 1...</td>\n",
       "      <td>germani economi biggest among countri share eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>300</td>\n",
       "      <td>J&amp;J agrees $25bn Guidant deal\\n</td>\n",
       "      <td>\\n Pharmaceutical giant Johnson &amp; Johnson has...</td>\n",
       "      <td>pharmaceut giant johnson johnson agre buy medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>1286</td>\n",
       "      <td>Child access law shake-up planned\\n</td>\n",
       "      <td>\\n Parents who refuse to allow former partner...</td>\n",
       "      <td>parent refus allow former partner contact chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>1506</td>\n",
       "      <td>Real in talks over Gravesen move\\n</td>\n",
       "      <td>\\n Real Madrid are closing in on a Β£2m deal ...</td>\n",
       "      <td>real madrid close deal everton thoma gravesen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2036</td>\n",
       "      <td>Lifestyle 'governs mobile choice'\\n</td>\n",
       "      <td>\\n Faster, better or funkier hardware alone i...</td>\n",
       "      <td>faster better funkier hardwar alon go help pho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                title  \\\n",
       "0     385    Tate & Lyle boss bags top award\\n   \n",
       "1    1984   Halo 2 sells five million copies\\n   \n",
       "2     986  MSPs hear renewed climate warning\\n   \n",
       "3    1387    Pavey focuses on indoor success\\n   \n",
       "4    1295   Tories reject rethink on axed MP\\n   \n",
       "..    ...                                  ...   \n",
       "440   439            German economy rebounds\\n   \n",
       "441   300      J&J agrees $25bn Guidant deal\\n   \n",
       "442  1286  Child access law shake-up planned\\n   \n",
       "443  1506   Real in talks over Gravesen move\\n   \n",
       "444  2036  Lifestyle 'governs mobile choice'\\n   \n",
       "\n",
       "                                               content  \\\n",
       "0     \\n Tate & Lyle's chief executive has been nam...   \n",
       "1     \\n Microsoft is celebrating bumper sales of i...   \n",
       "2     \\n Climate change could be completely out of ...   \n",
       "3     \\n Jo Pavey will miss January's View From Gre...   \n",
       "4     \\n Sacked MP Howard Flight's local Conservati...   \n",
       "..                                                 ...   \n",
       "440   \\n Germany's economy, the biggest among the 1...   \n",
       "441   \\n Pharmaceutical giant Johnson & Johnson has...   \n",
       "442   \\n Parents who refuse to allow former partner...   \n",
       "443   \\n Real Madrid are closing in on a Β£2m deal ...   \n",
       "444   \\n Faster, better or funkier hardware alone i...   \n",
       "\n",
       "                            content_clean_stems_lemmas  \n",
       "0    tate lyle chief execut name european businessm...  \n",
       "1    microsoft celebr bumper sale xbox scifi shoote...  \n",
       "2    climat chang could complet control within seve...  \n",
       "3    jo pavey miss januari view great edinburgh int...  \n",
       "4    sack mp howard flight local conserv associ ins...  \n",
       "..                                                 ...  \n",
       "440  germani economi biggest among countri share eu...  \n",
       "441  pharmaceut giant johnson johnson agre buy medi...  \n",
       "442  parent refus allow former partner contact chil...  \n",
       "443  real madrid close deal everton thoma gravesen ...  \n",
       "444  faster better funkier hardwar alon go help pho...  \n",
       "\n",
       "[445 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set  = pd.read_csv(\"./data_sets/test_set.tsv\", delimiter=\"\\t\",names=[\"id\",\"title\",\"content\"],           header=0)\n",
    "more_stop_words = [\"say\",\"said\",\"want\",\"thing\",\"may\",\"see\",\"make\",\"look\",\"likely\",\"well\",\"told\",\"uses\",\"used\",\"use\",\"bn\",\"mr\",\"year\",\"people\",\"new\"]\n",
    "pre = preprocessing()\n",
    "test_set = pre.clean_stem_lemmatize_column(test_set,\"content\",more_stop_words,True)\n",
    "display(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
