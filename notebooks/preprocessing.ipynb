{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\"\"\"\n",
    "clean_stem_lemmatize_tokens_column: full clean up for a column\n",
    "\n",
    "arguments:\n",
    "    dataframe:                pandas dataframe\n",
    "    column:                   string\n",
    "    more_stop_words:          string[]\n",
    "    drop_unnecessary_columns: bool\n",
    "\"\"\"\n",
    "def clean_stem_lemmatize_tokens_column(dataframe, column, more_stop_words = [], drop_unnecessary_columns = False):\n",
    "    clean_column(dataframe, column, more_stop_words)\n",
    "    stem_column(dataframe, f\"{column}_clean\")\n",
    "    lemmatize_column(dataframe, f\"{column}_clean_stems\")\n",
    "    tokenize_column(dataframe, f\"{column}_clean_stems_lemmas\")\n",
    "    if drop_unnecessary_columns is True:\n",
    "        dataframe = dataframe.drop([f\"{column}_clean\", f\"{column}_clean_stems\"], axis=1)\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "clean_tokens_stem_lemmatize_column: full clean up for a column\n",
    "\n",
    "arguments:\n",
    "    dataframe:                pandas dataframe\n",
    "    column:                   string\n",
    "    more_stop_words:          string[]\n",
    "    drop_unnecessary_columns: bool\n",
    "\"\"\"\n",
    "def clean_tokens_stem_lemmatize_column(dataframe, column, more_stop_words = [], drop_unnecessary_columns = False):\n",
    "    clean_column(dataframe, column, more_stop_words)\n",
    "    tokenize_column(dataframe, f\"{column}_clean\")\n",
    "    stem_column_tokens(dataframe, f\"{column}_clean_tokens\")\n",
    "    lemmatize_column_tokens(dataframe, f\"{column}_clean_tokens_stems\")\n",
    "    tokens_to_sentence_column(dataframe, f\"{column}_clean_tokens_stems_lemmas\")\n",
    "    if drop_unnecessary_columns is True:\n",
    "        dataframe = dataframe.drop([f\"{column}_clean\", f\"{column}_clean_tokens\", f\"{column}_clean_tokens_stems\"], axis=1)\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Tokenization\n",
    "\n",
    "Tokenization is the process of demarcating and possibly classifying sections \n",
    "of a string of input characters. The resulting tokens are then passed on to \n",
    "some other form of processing. The process can be considered a sub-task of \n",
    "parsing input.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tokenize_sentence: tokenizes a given sentence\n",
    "\n",
    "arguments:\n",
    "    sentence: string\n",
    "\"\"\"\n",
    "def tokenize_sentence(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "\n",
    "\"\"\"\n",
    "tokenize_column: tokenizes a column of a dataframe and saves it as column_tokens\n",
    "\n",
    "arguments:\n",
    "    dataframe: pandas dataframe\n",
    "    column:    string\n",
    "\"\"\"\n",
    "def tokenize_column(dataframe, column):\n",
    "    tokens = list()\n",
    "    # tokenize each element of the column\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence_tokens = tokenize_sentence(dataframe.loc[index,column])\n",
    "        tokens.append(sentence_tokens)\n",
    "    # save tokens as a new column\n",
    "    dataframe[f\"{column}_tokens\"] = pd.Series(tokens, index=dataframe.index)\n",
    "\n",
    "\"\"\"\n",
    "tokens_to_sentences: converts a list of tokens to a sentence\n",
    "\n",
    "arguments:\n",
    "    tokens: string[]\n",
    "\"\"\"\n",
    "def tokens_to_sentence(tokens):\n",
    "    sentence = \" \".join(tokens)\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\n",
    "tokens_to_sentence_column: converts a column of tokens to sentences and saves it as column_sentences\n",
    "\n",
    "arguments:\n",
    "    dataframe: pandas dataframe\n",
    "    column:    string\n",
    "\"\"\"\n",
    "def tokens_to_sentence_column(dataframe, column):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        dataframe.loc[index,f\"{column}_sentences\"] = tokens_to_sentence(dataframe.loc[index,column])\n",
    "\n",
    "\"\"\"\n",
    "clean_sentence: cleans a given sentence\n",
    "\n",
    "arguments:\n",
    "    sentence: string\n",
    "\"\"\"\n",
    "def clean_sentence(sentence):\n",
    "    sentence = re.sub(\"[^a-zA-z\\s]\", \"\", sentence) # remove special characters\n",
    "    sentence = re.sub(\"_\", \"\", sentence)\n",
    "    sentence = re.sub(\"\\s+\", \" \",sentence)         # change any white space to one space\n",
    "    sentence = sentence.strip()                    # remove start and end white spaces\n",
    "    sentence = sentence.lower()                    # convert sentence into lower case\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Stop words\n",
    "\n",
    "Stop words are the most common words in a language like \"the\", \"is\", \"a\". \n",
    "These words do not carry important meaning and are usually removed from texts.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "remove_stop_words_from_sentence: removes stop words fast using dictionary\n",
    "\n",
    "arguments:\n",
    "    sentence:        string[]\n",
    "    more_stop_words: string\n",
    "\"\"\"\n",
    "def remove_stop_words_from_sentence(sentence, more_stop_words = []):\n",
    "    stop_words = stopwords.words(\"english\") + more_stop_words\n",
    "    stopwords_dictionary = Counter(stop_words)\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in stopwords_dictionary])\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\n",
    "clean_column: cleans a dataframe column from symbols, removes stop words and saves it as column_clean\n",
    "\n",
    "arguments:\n",
    "    dataframe:       pandas dataframe\n",
    "    column:          string\n",
    "    more_stop_words: string[]\n",
    "\"\"\"\n",
    "def clean_column(dataframe, column, more_stop_words = []):\n",
    "    # clean and remove each element of the column\n",
    "    for index, row in dataframe.iterrows():\n",
    "        dataframe.loc[index,f\"{column}_clean\"] = clean_sentence(dataframe.loc[index,column])\n",
    "        dataframe.loc[index,f\"{column}_clean\"] = remove_stop_words_from_sentence(dataframe.loc[index,f\"{column}_clean\"], more_stop_words)\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words \n",
    "to their word stem, base or root formâ€”generally a written word form.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        \n",
    "\"\"\"\n",
    "stem_sentence: stems a given sentence\n",
    "\n",
    "arguments:\n",
    "    sentence: string\n",
    "\"\"\"\n",
    "def stem_sentence(sentence):\n",
    "    porter = PorterStemmer()\n",
    "    words = word_tokenize(sentence)\n",
    "    stems_tokens = list()\n",
    "    for word in words:\n",
    "        stems_tokens.append(porter.stem(word))\n",
    "    return tokens_to_sentence(stems_tokens)\n",
    "\n",
    "\"\"\"\n",
    "stem_column: stems a column of a dataframe and saves it as column_stems\n",
    "\n",
    "arguments:\n",
    "    dataframe: pandas dataframe\n",
    "    column:    string\n",
    "\"\"\"\n",
    "def stem_column(dataframe, column):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        dataframe.loc[index,f\"{column}_stems\"] = stem_sentence(dataframe.loc[index,column])\n",
    "\n",
    "\"\"\"\n",
    "stem_column_tokens: stems a column of tokens of a dataframe and saves it as column_stems\n",
    "\n",
    "arguments:\n",
    "    dataframe: pandas dataframe\n",
    "    column:    string\n",
    "\"\"\"\n",
    "def stem_column_tokens(dataframe, column):\n",
    "    porter = PorterStemmer()\n",
    "    stem_tokens = list()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        stem = list()\n",
    "        for token in dataframe.loc[index,column]:\n",
    "            stem.append(porter.stem(token))\n",
    "        stem_tokens.append(stem)\n",
    "    # save stemmed tokens as a new column\n",
    "    dataframe[f\"{column}_stems\"] = pd.Series(stem_tokens, index=dataframe.index)\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the \n",
    "inflected forms of a word so they can be analysed as a single item, identified by the \n",
    "word's lemma, or dictionary form.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "lemmatize_sentence: lemattizes a given sentence\n",
    "\n",
    "arguments:\n",
    "    sentence: string\n",
    "\"\"\"\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(sentence)\n",
    "    lemmas_tokens = list()\n",
    "    for word in words:\n",
    "        lemmas_tokens.append(lemmatizer.lemmatize(word))\n",
    "    return tokens_to_sentence(lemmas_tokens)\n",
    "\n",
    "\"\"\"\n",
    "lemmatize_sentence: lemattizes a column of a dataframe and saves it as column_lemmas\n",
    "\n",
    "arguments:\n",
    "    dataframe: pandas dataframe\n",
    "    column:    string\n",
    "\"\"\"\n",
    "def lemmatize_column(dataframe, column):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        dataframe.loc[index,f\"{column}_lemmas\"] = lemmatize_sentence(dataframe.loc[index,column])\n",
    "\n",
    "\"\"\"\n",
    "lemmatize_column_tokens: lemattizes a column of tokens of a dataframe and saves it as column_stems\n",
    "\n",
    "arguments:\n",
    "    dataframe: pandas dataframe\n",
    "    column:    string\n",
    "\"\"\"\n",
    "def lemmatize_column_tokens(dataframe, column):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_tokens = list()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        lemma = list()\n",
    "        for token in dataframe.loc[index,column]:\n",
    "            lemma.append(lemmatizer.lemmatize(token))\n",
    "        lemma_tokens.append(lemma)\n",
    "    # save lemmatized tokens as a new column\n",
    "    dataframe[f\"{column}_lemmas\"] = pd.Series(lemma_tokens, index=dataframe.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
