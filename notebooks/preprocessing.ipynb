{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.\n",
    "\n",
    "#### Stop words\n",
    "\n",
    "Stop words are the most common words in a language like \"the\", \"is\", \"a\". These words do not carry important meaning and are usually removed from texts.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formâ€”generally a written word form.\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class preprocessing:\n",
    "    \"\"\"\n",
    "    clean_stem_lemmatize_tokens_column: full clean up for a column\n",
    "\n",
    "    arguments:\n",
    "        dataframe:                pandas dataframe\n",
    "        column:                   string\n",
    "        more_stop_words:          string[]\n",
    "        drop_unnecessary_columns: bool\n",
    "    \"\"\"\n",
    "    def clean_stem_lemmatize_tokens_column(self, dataframe, column, more_stop_words = [], drop_unnecessary_columns = False):\n",
    "        self.clean_column(dataframe, column, more_stop_words)\n",
    "        self.stem_column(dataframe, f\"{column}_clean\")\n",
    "        self.lemmatize_column(dataframe, f\"{column}_clean_stems\")\n",
    "        self.tokenize_column(dataframe, f\"{column}_clean_stems_lemmas\")\n",
    "        if drop_unnecessary_columns is True:\n",
    "            dataframe = dataframe.drop([f\"{column}_clean\", f\"{column}_clean_stems\"], axis=1)\n",
    "        return dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    clean_tokens_stem_lemmatize_column: full clean up for a column\n",
    "\n",
    "    arguments:\n",
    "        dataframe:                pandas dataframe\n",
    "        column:                   string\n",
    "        more_stop_words:          string[]\n",
    "        drop_unnecessary_columns: bool\n",
    "    \"\"\"\n",
    "    def clean_tokens_stem_lemmatize_column(self, dataframe, column, more_stop_words = [], drop_unnecessary_columns = False):\n",
    "        self.clean_column(dataframe, column, more_stop_words)\n",
    "        self.tokenize_column(dataframe, f\"{column}_clean\")\n",
    "        self.stem_column_tokens(dataframe, f\"{column}_clean_tokens\")\n",
    "        self.lemmatize_column_tokens(dataframe, f\"{column}_clean_tokens_stems\")\n",
    "        self.tokens_to_sentence_column(dataframe, f\"{column}_clean_tokens_stems_lemmas\")\n",
    "        if drop_unnecessary_columns is True:\n",
    "            dataframe = dataframe.drop([f\"{column}_clean\", f\"{column}_clean_tokens\", f\"{column}_clean_tokens_stems\"], axis=1)\n",
    "        return dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    tokenize_sentence: tokenizes a given sentence\n",
    "\n",
    "    arguments:\n",
    "        sentence: string\n",
    "    \"\"\"\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        words = word_tokenize(sentence)\n",
    "        return words\n",
    "\n",
    "    \"\"\"\n",
    "    tokenize_column: tokenizes a column of a dataframe and saves it as column_tokens\n",
    "\n",
    "    arguments:\n",
    "        dataframe: pandas dataframe\n",
    "        column:    string\n",
    "    \"\"\"\n",
    "    def tokenize_column(self, dataframe, column):\n",
    "        tokens = list()\n",
    "        # tokenize each element of the column\n",
    "        for index, row in dataframe.iterrows():\n",
    "            sentence_tokens = self.tokenize_sentence(dataframe.loc[index,column])\n",
    "            tokens.append(sentence_tokens)\n",
    "        # save tokens as a new column\n",
    "        dataframe[f\"{column}_tokens\"] = pd.Series(tokens, index=dataframe.index)\n",
    "\n",
    "    \"\"\"\n",
    "    tokens_to_sentences: converts a list of tokens to a sentence\n",
    "\n",
    "    arguments:\n",
    "        tokens: string[]\n",
    "    \"\"\"\n",
    "    def tokens_to_sentence(self, tokens):\n",
    "        sentence = \" \".join(tokens)\n",
    "        return sentence\n",
    "\n",
    "    \"\"\"\n",
    "    tokens_to_sentence_column: converts a column of tokens to sentences and saves it as column_sentences\n",
    "\n",
    "    arguments:\n",
    "        dataframe: pandas dataframe\n",
    "        column:    string\n",
    "    \"\"\"\n",
    "    def tokens_to_sentence_column(self, dataframe, column):\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_sentences\"] = self.tokens_to_sentence(dataframe.loc[index,column])\n",
    "\n",
    "    \"\"\"\n",
    "    clean_sentence: cleans a given sentence\n",
    "\n",
    "    arguments:\n",
    "        sentence: string\n",
    "    \"\"\"\n",
    "    def clean_sentence(self, sentence):\n",
    "        sentence = re.sub(\"[^a-zA-z\\s]\", \"\", sentence) # remove special characters\n",
    "        sentence = re.sub(\"_\", \"\", sentence)\n",
    "        sentence = re.sub(\"\\s+\", \" \",sentence)         # change any white space to one space\n",
    "        sentence = sentence.strip()                    # remove start and end white spaces\n",
    "        sentence = sentence.lower()                    # convert sentence into lower case\n",
    "        return sentence\n",
    "\n",
    "    \"\"\"\n",
    "    remove_stop_words_from_sentence: removes stop words fast using dictionary\n",
    "\n",
    "    arguments:\n",
    "        sentence:        string[]\n",
    "        more_stop_words: string\n",
    "    \"\"\"\n",
    "    def remove_stop_words_from_sentence(self, sentence, more_stop_words = []):\n",
    "        stop_words = stopwords.words(\"english\") + more_stop_words\n",
    "        stopwords_dictionary = Counter(stop_words)\n",
    "        sentence = \" \".join([word for word in sentence.split() if word not in stopwords_dictionary])\n",
    "        return sentence\n",
    "\n",
    "    \"\"\"\n",
    "    clean_column: cleans a dataframe column from symbols, removes stop words and saves it as column_clean\n",
    "\n",
    "    arguments:\n",
    "        dataframe:       pandas dataframe\n",
    "        column:          string\n",
    "        more_stop_words: string[]\n",
    "    \"\"\"\n",
    "    def clean_column(self, dataframe, column, more_stop_words = []):\n",
    "        # clean and remove each element of the column\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_clean\"] = self.clean_sentence(dataframe.loc[index,column])\n",
    "            dataframe.loc[index,f\"{column}_clean\"] = self.remove_stop_words_from_sentence(dataframe.loc[index,f\"{column}_clean\"], more_stop_words)\n",
    "\n",
    "    \"\"\"\n",
    "    stem_sentence: stems a given sentence\n",
    "\n",
    "    arguments:\n",
    "        sentence: string\n",
    "    \"\"\"\n",
    "    def stem_sentence(self, sentence):\n",
    "        porter = PorterStemmer()\n",
    "        words = word_tokenize(sentence)\n",
    "        stems_tokens = list()\n",
    "        for word in words:\n",
    "            stems_tokens.append(porter.stem(word))\n",
    "        return self.tokens_to_sentence(stems_tokens)\n",
    "\n",
    "    \"\"\"\n",
    "    stem_column: stems a column of a dataframe and saves it as column_stems\n",
    "\n",
    "    arguments:\n",
    "        dataframe: pandas dataframe\n",
    "        column:    string\n",
    "    \"\"\"\n",
    "    def stem_column(self, dataframe, column):\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_stems\"] = self.stem_sentence(dataframe.loc[index,column])\n",
    "\n",
    "    \"\"\"\n",
    "    stem_column_tokens: stems a column of tokens of a dataframe and saves it as column_stems\n",
    "\n",
    "    arguments:\n",
    "        dataframe: pandas dataframe\n",
    "        column:    string\n",
    "    \"\"\"\n",
    "    def stem_column_tokens(self, dataframe, column):\n",
    "        porter = PorterStemmer()\n",
    "        stem_tokens = list()\n",
    "        for index, row in dataframe.iterrows():\n",
    "            stem = list()\n",
    "            for token in dataframe.loc[index,column]:\n",
    "                stem.append(porter.stem(token))\n",
    "            stem_tokens.append(stem)\n",
    "        # save stemmed tokens as a new column\n",
    "        dataframe[f\"{column}_stems\"] = pd.Series(stem_tokens, index=dataframe.index)\n",
    "\n",
    "    \"\"\"\n",
    "    lemmatize_sentence: lemattizes a given sentence\n",
    "\n",
    "    arguments:\n",
    "        sentence: string\n",
    "    \"\"\"\n",
    "    def lemmatize_sentence(self, sentence):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = word_tokenize(sentence)\n",
    "        lemmas_tokens = list()\n",
    "        for word in words:\n",
    "            lemmas_tokens.append(lemmatizer.lemmatize(word))\n",
    "        return self.tokens_to_sentence(lemmas_tokens)\n",
    "\n",
    "    \"\"\"\n",
    "    lemmatize_sentence: lemattizes a column of a dataframe and saves it as column_lemmas\n",
    "\n",
    "    arguments:\n",
    "        dataframe: pandas dataframe\n",
    "        column:    string\n",
    "    \"\"\"\n",
    "    def lemmatize_column(self, dataframe, column):\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataframe.loc[index,f\"{column}_lemmas\"] = self.lemmatize_sentence(dataframe.loc[index,column])\n",
    "\n",
    "    \"\"\"\n",
    "    lemmatize_column_tokens: lemattizes a column of tokens of a dataframe and saves it as column_stems\n",
    "\n",
    "    arguments:\n",
    "        dataframe: pandas dataframe\n",
    "        column:    string\n",
    "    \"\"\"\n",
    "    def lemmatize_column_tokens(self, dataframe, column):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemma_tokens = list()\n",
    "        for index, row in dataframe.iterrows():\n",
    "            lemma = list()\n",
    "            for token in dataframe.loc[index,column]:\n",
    "                lemma.append(lemmatizer.lemmatize(token))\n",
    "            lemma_tokens.append(lemma)\n",
    "        # save lemmatized tokens as a new column\n",
    "        dataframe[f\"{column}_lemmas\"] = pd.Series(lemma_tokens, index=dataframe.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
